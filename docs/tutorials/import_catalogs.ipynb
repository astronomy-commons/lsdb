{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26685ece166fa3f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Importing catalogs to HiPSCat format\n",
    "\n",
    "This notebook presents two modes of importing catalogs to HiPSCat format:\n",
    "\n",
    "1. `lsdb.from_dataframe()` method: helpful to load smaller catalogs from a single dataframe. data should have fewer than 1-2 million rows and the pandas dataframe should be less than 1-2G in-memory. if your data is larger, the format is complicated, you need more flexibility, or you notice any performance issues when importing with this mode, use the next mode.\n",
    "2. `hipscat-import` package: for large datasets (1G - 100s of TB). this is a purpose-built map-reduce pipeline for creating hipscat catalogs from various datasets. in this notebook, we use a very basic dataset and basic import options. please see [the full package documentation](https://hipscat-import.readthedocs.io/) if you need to do anything more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de4ed424644058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:55.941948Z",
     "start_time": "2023-11-10T15:14:55.912372Z"
    }
   },
   "outputs": [],
   "source": [
    "import lsdb\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d9e23",
   "metadata": {},
   "source": [
    "We will be importing `small_sky_order1` from a single CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f76fe439a32b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:55.946920Z",
     "start_time": "2023-11-10T15:14:55.920402Z"
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"small_sky_order1\"\n",
    "test_data_dir = Path.cwd() / \"..\" / \"..\" / \"tests\" / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37826f",
   "metadata": {},
   "source": [
    "Let's define the input and output paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e857888ae9ea70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:55.958272Z",
     "start_time": "2023-11-10T15:14:55.924340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input paths\n",
    "catalog_dir = test_data_dir / catalog_name\n",
    "catalog_csv_path = catalog_dir / f\"{catalog_name}.csv\"\n",
    "\n",
    "# Temporary directory for the intermediate/output files\n",
    "tmp_dir = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e74432b3437e2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## lsdb.from_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:55.987313Z",
     "start_time": "2023-11-10T15:14:55.934107Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read simple catalog from its CSV file\n",
    "catalog = lsdb.from_dataframe(\n",
    "    pd.read_csv(catalog_csv_path),\n",
    "    catalog_name=\"from_dataframe\",\n",
    "    catalog_type=\"object\",\n",
    "    lowest_order=2,\n",
    "    highest_order=5,\n",
    "    threshold=100,\n",
    ")\n",
    "\n",
    "# Save it to disk in HiPSCat format\n",
    "catalog.to_hipscat(f\"{tmp_dir.name}/from_dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cc2a7eac29dba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HiPSCat import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842520c",
   "metadata": {},
   "source": [
    "Let's install the latest release of hipscat-import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bca7773cf6b261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:57.130701Z",
     "start_time": "2023-11-10T15:14:55.985757Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/astronomy-commons/hipscat-import.git@main --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1324d21d62c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:57.134432Z",
     "start_time": "2023-11-10T15:14:57.131884Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from hipscat_import.catalog.arguments import ImportArguments\n",
    "from hipscat_import.pipeline import pipeline_with_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1538d8545265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:57.172913Z",
     "start_time": "2023-11-10T15:14:57.138071Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "args = ImportArguments(\n",
    "    ra_column=\"ra\",\n",
    "    dec_column=\"dec\",\n",
    "    lowest_healpix_order=2,\n",
    "    highest_healpix_order=5,\n",
    "    pixel_threshold=100,\n",
    "    file_reader=\"csv\",\n",
    "    input_file_list=[catalog_csv_path],\n",
    "    output_artifact_name=\"from_import_pipeline\",\n",
    "    output_path=tmp_dir.name,\n",
    "    resume=False,\n",
    ")\n",
    "\n",
    "with Client(n_workers=1) as client:\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffa1283ddbca39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's read both catalogs, from disk, and check that the two methods produced the same output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ff4e2b7ae291b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:15:01.288751Z",
     "start_time": "2023-11-10T15:15:01.264910Z"
    }
   },
   "outputs": [],
   "source": [
    "from_dataframe_catalog = lsdb.read_hipscat(f\"{tmp_dir.name}/from_dataframe\")\n",
    "from_dataframe_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575daf21cd4cfcad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:15:01.313421Z",
     "start_time": "2023-11-10T15:15:01.279523Z"
    }
   },
   "outputs": [],
   "source": [
    "from_import_pipeline_catalog = lsdb.read_hipscat(f\"{tmp_dir.name}/from_import_pipeline\")\n",
    "from_import_pipeline_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d176a8dc303aa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:15:01.334413Z",
     "start_time": "2023-11-10T15:15:01.289748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that the pixels they contain are similar\n",
    "assert from_dataframe_catalog.get_healpix_pixels() == from_import_pipeline_catalog.get_healpix_pixels()\n",
    "\n",
    "# Verify that resulting dataframes contain the same data\n",
    "sorted_from_dataframe = from_dataframe_catalog.compute().sort_index()\n",
    "sorted_from_import_pipeline = from_import_pipeline_catalog.compute().sort_index()\n",
    "pd.testing.assert_frame_equal(sorted_from_dataframe, sorted_from_import_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610371c",
   "metadata": {},
   "source": [
    "Finally, tear down the directory used for the intermediate / output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fcbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
