{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26685ece166fa3f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Importing catalogs to HATS format\n",
    "\n",
    "This notebook presents two modes of importing catalogs to HATS format:\n",
    "\n",
    "1. The `lsdb.from_dataframe()` method is useful for loading smaller catalogs from a single DataFrame. The data should have fewer than 1-2 million rows, and the pandas DataFrame should occupy less than 1-2 GB in memory. If your data is larger, has a complex format, requires greater flexibility, or if you encounter performance issues with this method, consider using the next mode.\n",
    "2. The hats-import package is designed for large datasets (from 1 GB to hundreds of terabytes). This is a purpose-built map-reduce pipeline for creating HATS catalogs from various datasets. in this notebook, we use a very basic dataset and simple import options. Please see [the full package documentation](https://hats-import.readthedocs.io/) if you need to do anything more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de4ed424644058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:55.941948Z",
     "start_time": "2023-11-10T15:14:55.912372Z"
    }
   },
   "outputs": [],
   "source": [
    "import lsdb\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d9e23",
   "metadata": {},
   "source": [
    "We will be importing `small_sky` from a single CSV file. (If you did not install `lsdb` from source, you can find the file [here](https://github.com/astronomy-commons/lsdb/blob/main/tests/data/raw/small_sky/small_sky.csv) and modify the paths accordingly.)\n",
    "\n",
    "Let's define the input and output paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f76fe439a32b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:55.946920Z",
     "start_time": "2023-11-10T15:14:55.920402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input paths\n",
    "test_data_dir = Path.cwd() / \"..\" / \"..\" / \"tests\" / \"data\"\n",
    "catalog_csv_path = test_data_dir / \"raw\" / \"small_sky\" / \"small_sky.csv\"\n",
    "\n",
    "# Temporary directory for the intermediate/output files\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "tmp_path = Path(tmp_dir.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e74432b3437e2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## lsdb.from_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:55.987313Z",
     "start_time": "2023-11-10T15:14:55.934107Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read simple catalog from its CSV file\n",
    "catalog = lsdb.from_dataframe(\n",
    "    pd.read_csv(catalog_csv_path),\n",
    "    catalog_name=\"from_dataframe\",\n",
    "    catalog_type=\"object\",\n",
    "    lowest_order=2,\n",
    "    highest_order=5,\n",
    "    threshold=100,\n",
    ")\n",
    "\n",
    "# Save it to disk in HATS format\n",
    "catalog.to_hats(tmp_path / \"from_dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cc2a7eac29dba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HATS import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842520c",
   "metadata": {},
   "source": [
    "Please uncomment the next line to install the latest release of hats-import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bca7773cf6b261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:57.130701Z",
     "start_time": "2023-11-10T15:14:55.985757Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/astronomy-commons/hats-import.git@main --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1324d21d62c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:57.134432Z",
     "start_time": "2023-11-10T15:14:57.131884Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "from hats_import.pipeline import pipeline_with_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1538d8545265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:14:57.172913Z",
     "start_time": "2023-11-10T15:14:57.138071Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "args = ImportArguments(\n",
    "    ra_column=\"ra\",\n",
    "    dec_column=\"dec\",\n",
    "    lowest_healpix_order=2,\n",
    "    highest_healpix_order=5,\n",
    "    pixel_threshold=100,\n",
    "    file_reader=\"csv\",\n",
    "    input_file_list=[catalog_csv_path],\n",
    "    output_artifact_name=\"from_import_pipeline\",\n",
    "    output_path=tmp_path,\n",
    "    resume=False,\n",
    ")\n",
    "\n",
    "with Client(n_workers=1) as client:\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffa1283ddbca39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's read both catalogs (from disk) and check that the two methods produced the same output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ff4e2b7ae291b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:15:01.288751Z",
     "start_time": "2023-11-10T15:15:01.264910Z"
    }
   },
   "outputs": [],
   "source": [
    "from_dataframe_catalog = lsdb.read_hats(tmp_path / \"from_dataframe\")\n",
    "from_dataframe_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575daf21cd4cfcad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:15:01.313421Z",
     "start_time": "2023-11-10T15:15:01.279523Z"
    }
   },
   "outputs": [],
   "source": [
    "from_import_pipeline_catalog = lsdb.read_hats(tmp_path / \"from_import_pipeline\")\n",
    "from_import_pipeline_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d176a8dc303aa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:15:01.334413Z",
     "start_time": "2023-11-10T15:15:01.289748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that the pixels they contain are similar\n",
    "assert from_dataframe_catalog.get_healpix_pixels() == from_import_pipeline_catalog.get_healpix_pixels()\n",
    "\n",
    "# Verify that resulting dataframes contain the same data\n",
    "sorted_from_dataframe = from_dataframe_catalog.compute().sort_index()\n",
    "sorted_from_import_pipeline = from_import_pipeline_catalog.compute().sort_index()\n",
    "pd.testing.assert_frame_equal(sorted_from_dataframe, sorted_from_import_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610371c",
   "metadata": {},
   "source": [
    "Finally, tear down the directory used for the intermediate / output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fcbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
