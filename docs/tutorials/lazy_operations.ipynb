{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abd9abf6ebf0b62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lazy Operations in LSDB\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will learn:\n",
    "\n",
    "* What are lazy operations and how LSDB uses them to run pipelines at scale.\n",
    "* How to preview a small part of the data.\n",
    "\n",
    "## Introduction - What are Lazy Operations?\n",
    "\n",
    "In the previous tutorial we looked at loading a catalog and inspecting it's metadata. When we call `open_catalog()`, only the catalog's metadata is loaded, not any of the data in the rows of the catalog. This is because operations in LSDB are *lazy*: when you call the operation it isn't actually executed immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6e117e9c8603f",
   "metadata": {},
   "source": [
    "<video src=\"../_static/lazy-flowchart.mp4\" loop autoplay controls style=\"width: 100%;\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4990b67a880db78",
   "metadata": {},
   "source": [
    "As explained in the video above, instead of executing the operation when you call it, the operation is just planned. The catalog object keeps track of the pipeline of operations by building a task graph - an object that keeps track of the pipeline of operations you want to perform on the catalog. This way, you can write the code for the pipeline locally, and the task graph will be sent to the workers to execute the pipeline in parallel. We can also perform optimizations to the task graph to make sure the workflow is as efficient as possible. This is how LSDB can scale from working on your local machine, to running pipelines on clusters or in the cloud without having to make any code changes.\n",
    "\n",
    "To actually execute the operations, you call the `catalog.compute()` method, which will execute the pipeline and return the resulting data as a pandas `DataFrame`.\n",
    "\n",
    "You will find that most use cases start with **LAZY** loading and planning operations, followed by more expensive **COMPUTE** operations. The data is only loaded into memory when we trigger the workflow computations, usually with a `compute` call.\n",
    "\n",
    "![Lazy workflow diagram](../_static/lazy_diagram.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749eddc3e80924f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T22:23:25.133550Z",
     "start_time": "2025-06-27T22:23:22.314574Z"
    }
   },
   "outputs": [],
   "source": [
    "import lsdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3132659ff56e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T22:23:37.735159Z",
     "start_time": "2025-06-27T22:23:30.150510Z"
    }
   },
   "outputs": [],
   "source": [
    "gaia = lsdb.open_catalog(\"https://data.lsdb.io/hats/gaia_dr3/gaia/\")\n",
    "gaia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4c3508cedd99",
   "metadata": {},
   "source": [
    "We can see above from the `...` as placeholders for the data, and the warning at the bottom that this catalog has been loaded lazily. Now that we have the object we're ready to start performing operations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf6067746a17db2",
   "metadata": {},
   "source": [
    "## Operating on the Catalog\n",
    "\n",
    "Once we have a catalog object, we can start planning operations on it. In the rest of the tutorials, we'll look deeper into exactly what kind of operations you can do with a catalog. The catalog is based on pandas `DataFrames` so you'll see some functions that work the same as in pandas, such as `columns`, `dtypes`, `query`, and selecting columns or filtering with `[]`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ba4e842974cfeeb",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There are also a lot of astronomy specific functions, such as :doc:`Spatial filters like cone search or box search </tutorials/region_selection>`, and :doc:`Crossmatching </tutorials/pre_executed/crossmatching>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d918084e7e4b7ff",
   "metadata": {},
   "source": [
    "After you've performed your operations, you can call `catalog.compute()` to perform the pipeline, but this will run on the entire catalog!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc401709e9c59a64",
   "metadata": {},
   "source": [
    "## Previewing part of the data\n",
    "\n",
    "Computing an entire catalog will result in loading all of its data into memory on your local machine after the workers have computed it, which is expensive and may lead to out-of-memory issues.\n",
    "\n",
    "Often, our goal is to have a peek at a slice of data to make sure the workflow output is reasonable (e.g., to assess if some new created columns are present and their values have been properly processed). `head()` is a pandas-like method which allows us to preview part of the data for this purpose. It runs the pipeline on the catalog partitions one by one, and finds the first few rows of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23fe9dd7ee87d2",
   "metadata": {},
   "source": [
    "### Making a Dask client\n",
    "\n",
    "LSDB is built on top of the [Dask](https://www.dask.org) framework, which allows the pipelines to be executed on distributed workers. Before we do anything that executes the pipeline such as `head()` or `compute()`, we recommend making a dask client."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f62f62b60d8853fc",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For additional information on dask client creation, please read our tutorial on :doc:`Setting up a Dask Client </tutorials/dask_client>`.\n",
    "\n",
    "For now, we'll make a simple Client that uses 4 workers on our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f779dce03051a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T22:26:46.618684Z",
     "start_time": "2025-06-27T22:26:44.117882Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4, memory_limit=\"auto\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de91d440baf189a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T22:29:12.566661Z",
     "start_time": "2025-06-27T22:26:54.923044Z"
    }
   },
   "outputs": [],
   "source": [
    "gaia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5141e308183b854",
   "metadata": {},
   "source": [
    "By default, the first 5 rows of data will be shown, but we can specify a higher number if we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d13d749abc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01bd4d0a456faa",
   "metadata": {},
   "source": [
    "### Closing the Dask client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8fd20f7e9a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9c4a4b7cfeb65",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "**Authors**: Sean McGuire\n",
    "\n",
    "**Last updated on**: June 27, 2025\n",
    "\n",
    "If you use `lsdb` for published research, please cite following [instructions](https://docs.lsdb.io/en/stable/citation.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
